{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:39:35.523232Z",
     "start_time": "2021-01-13T10:39:35.517635Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import  sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:33:28.192047Z",
     "start_time": "2021-01-13T08:33:27.630814Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HANBIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.185864Z",
     "start_time": "2021-01-13T08:04:32.083653Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "train = pd.read_csv('../data/spooky-author-identification/train/train.csv')\n",
    "test = pd.read_csv('../data/spooky-author-identification/test/test.csv')\n",
    "sample = pd.read_csv('../data/spooky-author-identification/sample_submission/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.207792Z",
     "start_time": "2021-01-13T08:04:32.187859Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.218756Z",
     "start_time": "2021-01-13T08:04:32.210784Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.231717Z",
     "start_time": "2021-01-13T08:04:32.220749Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "우리의 목표는 text를 분석하여 EAP, HPL, MWS와 같은 저자를 예측해야 한다. 간단히 말해서, 3개의 다른 클래스로 텍스트 분류하는 것이다.\n",
    "해결방법은 캐글은 multi-class log-loss를 평가 지표를 지정하였다. 자세한 것은 [해당 사이트](https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py) 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.240682Z",
     "start_time": "2021-01-13T08:04:32.233707Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param prdicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    if len (actual.shape) == 1: # 실제값이 하나라면 배열 다시 생성.\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual= actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1-eps)\n",
    "    rows = actual.shape[0]\n",
    "    vstoa = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vstoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.254636Z",
     "start_time": "2021-01-13T08:04:32.245665Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit learn의 LabelEncoder를 해보자. 0,1,2로 변환할 것이다.\n",
    "\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.273573Z",
     "start_time": "2021-01-13T08:04:32.257627Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 그리고, 데이터셋을 training 데이터와 검증 데이터 셋으로 나눠준다. scikit-learn의 model_selection을 사용하여 나눠준다.\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, stratify=y,\n",
    "                                                  random_state=42, test_size=0.1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:32.283539Z",
     "start_time": "2021-01-13T08:04:32.275566Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n",
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print( xtrain.shape)\n",
    "print( xvalid.shape)\n",
    "\n",
    "print( ytrain.shape)\n",
    "print( yvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 기본적인 모델 세우기\n",
    "### TF-IDF\n",
    "첫번째 모델은 TF-IDF(Term Frequency - Inverse Document Frequency)이다. 이것은 간단한 Logistic Regression을 따른다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:33.742668Z",
     "start_time": "2021-01-13T08:04:32.285535Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hair': 5920,\n",
       " 'brightest': 1489,\n",
       " 'living': 7734,\n",
       " 'gold': 5641,\n",
       " 'despite': 3312,\n",
       " 'poverty': 10050,\n",
       " 'clothing': 2161,\n",
       " 'set': 11813,\n",
       " 'crown': 2813,\n",
       " 'distinction': 3647,\n",
       " 'head': 6078,\n",
       " 'said': 11446,\n",
       " 'oh': 9156,\n",
       " 'member': 8268,\n",
       " 'family': 4746,\n",
       " 'niece': 8899,\n",
       " 'accomplished': 72,\n",
       " 'woman': 14862,\n",
       " 'magistrate': 7995,\n",
       " 'appeared': 537,\n",
       " 'perfectly': 9656,\n",
       " 'continued': 2559,\n",
       " 'attentive': 814,\n",
       " 'interested': 6950,\n",
       " 'saw': 11546,\n",
       " 'shudder': 12014,\n",
       " 'horror': 6340,\n",
       " 'lively': 7729,\n",
       " 'surprise': 13042,\n",
       " 'unmingled': 14071,\n",
       " 'disbelief': 3534,\n",
       " 'painted': 9422,\n",
       " 'countenance': 2683,\n",
       " 'horrible': 6335,\n",
       " 'eyes': 4657,\n",
       " 'blacker': 1239,\n",
       " 'seared': 11684,\n",
       " 'face': 4688,\n",
       " 'opened': 9245,\n",
       " 'wide': 14739,\n",
       " 'expression': 4594,\n",
       " 'unable': 13912,\n",
       " 'interpret': 6965,\n",
       " 'longer': 7805,\n",
       " 'bent': 1159,\n",
       " 'ground': 5837,\n",
       " 'like': 7638,\n",
       " 'nursed': 9023,\n",
       " 'flower': 5088,\n",
       " 'spring': 12530,\n",
       " 'shooting': 11954,\n",
       " 'strength': 12769,\n",
       " 'weighed': 14644,\n",
       " 'blossoms': 1292,\n",
       " 'does': 3699,\n",
       " 'lord': 7836,\n",
       " 'say': 11563,\n",
       " 'falling': 4730,\n",
       " 'discord': 3555,\n",
       " 'concord': 2394,\n",
       " 'great': 5741,\n",
       " 'sweetness': 13101,\n",
       " 'music': 8710,\n",
       " 'hath': 6042,\n",
       " 'agreement': 281,\n",
       " 'affections': 234,\n",
       " 'better': 1187,\n",
       " 'brown': 1536,\n",
       " 'jenkin': 7092,\n",
       " 'rubbing': 11281,\n",
       " 'kind': 7230,\n",
       " 'affectionate': 233,\n",
       " 'ankles': 462,\n",
       " 'black': 1230,\n",
       " 'man': 8051,\n",
       " 'deep': 3123,\n",
       " 'mud': 8676,\n",
       " 'largely': 7390,\n",
       " 'concealed': 2368,\n",
       " 'brown jenkin': 1537,\n",
       " 'black man': 1234,\n",
       " 'hours': 6369,\n",
       " 'meditate': 8242,\n",
       " 'till': 13535,\n",
       " 'hunger': 6449,\n",
       " 'fatigue': 4831,\n",
       " 'brought': 1530,\n",
       " 'passing': 9542,\n",
       " 'hour': 6362,\n",
       " 'marked': 8136,\n",
       " 'long': 7782,\n",
       " 'shadows': 11853,\n",
       " 'cast': 1773,\n",
       " 'descending': 3272,\n",
       " 'sun': 12949,\n",
       " 'descending sun': 3273,\n",
       " 'time': 13548,\n",
       " 'pierre': 9798,\n",
       " 'imposed': 6622,\n",
       " 'noble': 8937,\n",
       " 'birth': 1215,\n",
       " 'placed': 9850,\n",
       " 'association': 744,\n",
       " 'company': 2317,\n",
       " 'weeping': 14642,\n",
       " 'clung': 2176,\n",
       " 'cried': 2783,\n",
       " 'unkind': 14056,\n",
       " 'lionel': 7683,\n",
       " 'pretty': 10166,\n",
       " 'miss': 8435,\n",
       " 'received': 10686,\n",
       " 'visits': 14433,\n",
       " 'approaching': 572,\n",
       " 'marriage': 8141,\n",
       " 'young': 15058,\n",
       " 'englishman': 4253,\n",
       " 'john': 7110,\n",
       " 'esq': 4376,\n",
       " 'struck': 12809,\n",
       " 'space': 12418,\n",
       " 'elapsed': 4072,\n",
       " 'madness': 7984,\n",
       " 'reasonable': 10669,\n",
       " 'impulse': 6650,\n",
       " 'regulated': 10789,\n",
       " 'actions': 123,\n",
       " 'space time': 12419,\n",
       " 'time elapsed': 13552,\n",
       " 'did': 3400,\n",
       " 'proceed': 10231,\n",
       " 'stairs': 12576,\n",
       " 'relation': 10807,\n",
       " 'french': 5270,\n",
       " 'ravings': 10617,\n",
       " 'rhoby': 11101,\n",
       " 'harris': 6022,\n",
       " 'inhabitants': 6809,\n",
       " 'shunned': 12020,\n",
       " 'house': 6374,\n",
       " 'imagination': 6544,\n",
       " 'future': 5365,\n",
       " 'discovery': 3565,\n",
       " 'determine': 3346,\n",
       " 'rhoby harris': 11102,\n",
       " 'shunned house': 12021,\n",
       " 'sitting': 12138,\n",
       " 'chest': 1967,\n",
       " 'spices': 12486,\n",
       " 'sat': 11521,\n",
       " 'realise': 10649,\n",
       " 'passed': 9529,\n",
       " 'time passed': 13563,\n",
       " 'faint': 4715,\n",
       " 'fourth': 5231,\n",
       " 'handle': 5967,\n",
       " 'fumes': 5334,\n",
       " 'begun': 1097,\n",
       " 'penetrate': 9625,\n",
       " 'mask': 8162,\n",
       " 'recovered': 10720,\n",
       " 'hole': 6289,\n",
       " 'emitting': 4160,\n",
       " 'fresh': 5278,\n",
       " 'vapours': 14264,\n",
       " 'calmly': 1665,\n",
       " 'left': 7499,\n",
       " 'little': 7705,\n",
       " 'previously': 10179,\n",
       " 'called': 1653,\n",
       " 'shaky': 11862,\n",
       " 'whisper': 14716,\n",
       " 'portentous': 10001,\n",
       " 'loudest': 7865,\n",
       " 'shriek': 11992,\n",
       " 'god': 5623,\n",
       " 'seeing': 11723,\n",
       " 'answer': 479,\n",
       " 'occupied': 9115,\n",
       " 'arranging': 645,\n",
       " 'cottage': 2668,\n",
       " 'old': 9168,\n",
       " 'walked': 14520,\n",
       " 'minutes': 8405,\n",
       " 'leaning': 7466,\n",
       " 'arm': 628,\n",
       " 'youth': 15081,\n",
       " 'young woman': 15077,\n",
       " 'occupied arranging': 9116,\n",
       " 'old man': 9187,\n",
       " 'leaning arm': 7467,\n",
       " 'flies': 5063,\n",
       " 'caught': 1799,\n",
       " 'high': 6231,\n",
       " 'born': 1375,\n",
       " 'powerful': 10054,\n",
       " 'bow': 1404,\n",
       " 'necks': 8831,\n",
       " 'flimsy': 5068,\n",
       " 'unmeaning': 14069,\n",
       " 'pretensions': 10164,\n",
       " 'high born': 6232,\n",
       " 'ape': 517,\n",
       " 'approached': 570,\n",
       " 'casement': 1767,\n",
       " 'mutilated': 8728,\n",
       " 'burden': 1578,\n",
       " 'sailor': 11485,\n",
       " 'shrank': 11990,\n",
       " 'aghast': 266,\n",
       " 'rod': 11210,\n",
       " 'gliding': 5586,\n",
       " 'hurried': 6457,\n",
       " 'home': 6298,\n",
       " 'dreading': 3824,\n",
       " 'consequences': 2480,\n",
       " 'butchery': 1618,\n",
       " 'gladly': 5560,\n",
       " 'abandoning': 2,\n",
       " 'terror': 13327,\n",
       " 'solicitude': 12323,\n",
       " 'fate': 4817,\n",
       " 'ourang': 9335,\n",
       " 'outang': 9337,\n",
       " 'ourang outang': 9336,\n",
       " 'reward': 11097,\n",
       " 'promised': 10298,\n",
       " 'detested': 3351,\n",
       " 'toils': 13608,\n",
       " 'consolation': 2499,\n",
       " 'unparalleled': 14090,\n",
       " 'sufferings': 12911,\n",
       " 'prospect': 10342,\n",
       " 'day': 2993,\n",
       " 'miserable': 8423,\n",
       " 'slavery': 12180,\n",
       " 'claim': 2066,\n",
       " 'elizabeth': 4112,\n",
       " 'forget': 5178,\n",
       " 'past': 9551,\n",
       " 'union': 14042,\n",
       " 'think': 13396,\n",
       " 'groans': 5825,\n",
       " 'clerval': 2109,\n",
       " 'ears': 3980,\n",
       " 'beloved': 1135,\n",
       " 'father': 4820,\n",
       " 'words': 14896,\n",
       " 'truly': 13822,\n",
       " 'forgive': 5181,\n",
       " 'entirely': 4313,\n",
       " 'possess': 10018,\n",
       " 'heart': 6119,\n",
       " 'endeavoured': 4219,\n",
       " 'rainbow': 10552,\n",
       " 'gleams': 5578,\n",
       " 'cataract': 1791,\n",
       " 'd': 2899,\n",
       " 'soften': 12300,\n",
       " 'thy': 13514,\n",
       " 'tremendous': 13776,\n",
       " 'sorrows': 12377,\n",
       " 'oh beloved': 9157,\n",
       " 'beloved father': 1136,\n",
       " 'interior': 6955,\n",
       " 'apparently': 530,\n",
       " 'details': 3334,\n",
       " 'constantly': 2506,\n",
       " 'changing': 1900,\n",
       " 'certain': 1861,\n",
       " 'faces': 4692,\n",
       " 'furniture': 5353,\n",
       " 'room': 11231,\n",
       " 'doors': 3751,\n",
       " 'windows': 14797,\n",
       " 'just': 7160,\n",
       " 'state': 12618,\n",
       " 'presumably': 10157,\n",
       " 'mobile': 8465,\n",
       " 'objects': 9070,\n",
       " 'house old': 6384,\n",
       " 'old house': 9182,\n",
       " 'close': 2135,\n",
       " 'spot': 12517,\n",
       " 'stood': 12704,\n",
       " 'solitary': 12326,\n",
       " 'rock': 11205,\n",
       " 'conical': 2450,\n",
       " 'divided': 3678,\n",
       " 'mountain': 8623,\n",
       " 'nature': 8797,\n",
       " 'hewn': 6216,\n",
       " 'pyramid': 10458,\n",
       " 'labour': 7319,\n",
       " 'block': 1280,\n",
       " 'reduced': 10735,\n",
       " 'perfect': 9650,\n",
       " 'shape': 11886,\n",
       " 'narrow': 8772,\n",
       " 'cell': 1833,\n",
       " 'beneath': 1144,\n",
       " 'raymond': 10621,\n",
       " 'short': 11964,\n",
       " 'inscription': 6863,\n",
       " 'carved': 1762,\n",
       " 'stone': 12694,\n",
       " 'recorded': 10716,\n",
       " 'tenant': 13284,\n",
       " 'cause': 1801,\n",
       " 'death': 3052,\n",
       " 'rock high': 11206,\n",
       " 's': 11341,\n",
       " 'march': 8122,\n",
       " 'mountains': 8627,\n",
       " 'snowy': 12279,\n",
       " 'come': 2251,\n",
       " 'health': 6095,\n",
       " 'committing': 2292,\n",
       " 'loved': 7877,\n",
       " 'ones': 9221,\n",
       " 'charge': 1916,\n",
       " 'tree': 13770,\n",
       " 'humanity': 6432,\n",
       " 'send': 11764,\n",
       " 'late': 7400,\n",
       " 'posterity': 10031,\n",
       " 'tale': 13189,\n",
       " 'ante': 485,\n",
       " 'pestilential': 9736,\n",
       " 'race': 10525,\n",
       " 'heroes': 6206,\n",
       " 'sages': 11444,\n",
       " 'lost': 7853,\n",
       " 'things': 13380,\n",
       " 'close day': 2136,\n",
       " 'day s': 3008,\n",
       " 'state things': 12622,\n",
       " 'rim': 11151,\n",
       " 'screwed': 11643,\n",
       " 'large': 7384,\n",
       " 'tube': 13838,\n",
       " 'condenser': 2402,\n",
       " 'body': 1328,\n",
       " 'machine': 7956,\n",
       " 'course': 2704,\n",
       " 'chamber': 1883,\n",
       " 'gum': 5883,\n",
       " 'elastic': 4074,\n",
       " 'gum elastic': 5885,\n",
       " 'feel': 4876,\n",
       " 'inconvenience': 6703,\n",
       " 'weather': 14624,\n",
       " 'busy': 1613,\n",
       " 'scenes': 11601,\n",
       " 'evil': 4444,\n",
       " 'despair': 3302,\n",
       " 'did feel': 3416,\n",
       " 'soon': 12350,\n",
       " 'conquered': 2466,\n",
       " 'latent': 7407,\n",
       " 'distaste': 3640,\n",
       " 'watch': 14575,\n",
       " 'perdita': 9644,\n",
       " 'mind': 8380,\n",
       " 'thing': 13370,\n",
       " 'heard': 6102,\n",
       " 'roughly': 11254,\n",
       " 'study': 12827,\n",
       " 'desire': 3293,\n",
       " 'wisest': 14824,\n",
       " 'men': 8275,\n",
       " 'creation': 2761,\n",
       " 'world': 14915,\n",
       " 'grasp': 5717,\n",
       " 'wisest men': 14825,\n",
       " 'unintelligible': 14040,\n",
       " 'matters': 8206,\n",
       " 'justly': 7176,\n",
       " 'instructed': 6912,\n",
       " 'regard': 10771,\n",
       " 'supreme': 13015,\n",
       " 'end': 4207,\n",
       " 'month': 8537,\n",
       " 'suddenly': 12899,\n",
       " 'quitted': 10508,\n",
       " 'sic': 12033,\n",
       " 'servant': 11805,\n",
       " 'departed': 3230,\n",
       " 'country': 2694,\n",
       " 'word': 14893,\n",
       " 'writing': 14977,\n",
       " 'informing': 6799,\n",
       " 'intentions': 6945,\n",
       " 'quitted house': 10510,\n",
       " 'cohort': 2202,\n",
       " 'remained': 10836,\n",
       " 'torches': 13651,\n",
       " 'faded': 4705,\n",
       " 'watched': 14576,\n",
       " 'thought': 13434,\n",
       " 'fantastic': 4767,\n",
       " 'sky': 12160,\n",
       " 'spectral': 12462,\n",
       " 'luminosity': 7907,\n",
       " 'flowed': 5087,\n",
       " 'determined': 3347,\n",
       " 'depart': 3229,\n",
       " 'live': 7725,\n",
       " 'leave': 7480,\n",
       " 'continue': 2558,\n",
       " 'exist': 4536,\n",
       " 'drop': 3866,\n",
       " 'enigmas': 4260,\n",
       " 'resolved': 10981,\n",
       " 'let': 7549,\n",
       " 'ensue': 4282,\n",
       " 'force': 5153,\n",
       " 'passage': 9526,\n",
       " 'moon': 8548,\n",
       " 'resolved let': 10982,\n",
       " 'force passage': 5154,\n",
       " 'fact': 4697,\n",
       " 'examine': 4466,\n",
       " 'causes': 1803,\n",
       " 'life': 7588,\n",
       " 'recourse': 10718,\n",
       " 'bitter': 1224,\n",
       " 'swiftly': 13108,\n",
       " 'shadowed': 11851,\n",
       " 'joy': 7130,\n",
       " 'endeavouring': 4220,\n",
       " 'master': 8179,\n",
       " 'passion': 9544,\n",
       " 'tears': 13245,\n",
       " 'threatened': 13471,\n",
       " 'eyes ground': 4670,\n",
       " 'automaton': 863,\n",
       " 'piece': 9792,\n",
       " 'distinct': 3644,\n",
       " 'motion': 8605,\n",
       " 'observable': 9085,\n",
       " 'shoulder': 11977,\n",
       " 'slight': 12202,\n",
       " 'degree': 3167,\n",
       " 'drapery': 3807,\n",
       " 'covering': 2720,\n",
       " 'just beneath': 7161,\n",
       " 'left shoulder': 7510,\n",
       " 'slight degree': 12203,\n",
       " 'grew': 5803,\n",
       " 'sterner': 12670,\n",
       " 'elderly': 4081,\n",
       " 'eager': 3954,\n",
       " 'questions': 10490,\n",
       " 'fell': 4896,\n",
       " 'involuntarily': 7021,\n",
       " 'convulsed': 2611,\n",
       " 'lips': 7686,\n",
       " 'plague': 9859,\n",
       " 'insistent': 6882,\n",
       " 'unendurable': 13997,\n",
       " 'cacophony': 1637,\n",
       " 'constant': 2504,\n",
       " 'terrifying': 13325,\n",
       " 'impression': 6634,\n",
       " 'sounds': 12399,\n",
       " 'regions': 10779,\n",
       " 'trembling': 13775,\n",
       " 'brink': 1506,\n",
       " 'different': 3462,\n",
       " 'interests': 6952,\n",
       " 'absorbed': 38,\n",
       " 'single': 12100,\n",
       " 'contemplation': 2540,\n",
       " 'horrors': 6344,\n",
       " 'reached': 10630,\n",
       " 'relate': 10803,\n",
       " 'tedious': 13251,\n",
       " 'educated': 4037,\n",
       " 'unusual': 14145,\n",
       " 'powers': 10056,\n",
       " 'infected': 6773,\n",
       " 'misanthropy': 8420,\n",
       " 'subject': 12855,\n",
       " 'perverse': 9730,\n",
       " 'moods': 8546,\n",
       " 'alternate': 373,\n",
       " 'enthusiasm': 4307,\n",
       " 'melancholy': 8261,\n",
       " 'powers mind': 10058,\n",
       " 'chase': 1932,\n",
       " 'nameless': 8757,\n",
       " 'entity': 4316,\n",
       " 'quite': 10502,\n",
       " 'trifling': 13796,\n",
       " 'difficulty': 3467,\n",
       " 'bag': 945,\n",
       " 'attorney': 822,\n",
       " 'adrian': 189,\n",
       " 'introduced': 6994,\n",
       " 'systematic': 13148,\n",
       " 'modes': 8479,\n",
       " 'proceeding': 10234,\n",
       " 'metropolis': 8344,\n",
       " 'stop': 12714,\n",
       " 'progress': 10280,\n",
       " 'prevented': 10174,\n",
       " 'evils': 4448,\n",
       " 'vice': 14359,\n",
       " 'folly': 5130,\n",
       " 'rendering': 10875,\n",
       " 'awful': 906,\n",
       " 'ordinary': 9292,\n",
       " 'parisian': 9490,\n",
       " 'gateway': 5433,\n",
       " 'glazed': 5574,\n",
       " 'box': 1411,\n",
       " 'sliding': 12201,\n",
       " 'window': 14793,\n",
       " 'indicating': 6733,\n",
       " 'forgot': 5183,\n",
       " 'distance': 3635,\n",
       " 'thee': 13350,\n",
       " 'eye': 4646,\n",
       " 'removed': 10869,\n",
       " 'glass': 5570,\n",
       " 'scarce': 11585,\n",
       " 'discern': 3537,\n",
       " 'forms': 5200,\n",
       " 'crowd': 2810,\n",
       " 'mile': 8367,\n",
       " 'surrounded': 13048,\n",
       " 'gate': 5430,\n",
       " 'form': 5186,\n",
       " 'learned': 7473,\n",
       " 'knowing': 7293,\n",
       " 'advanced': 195,\n",
       " 'age': 258,\n",
       " 'journey': 7125,\n",
       " 'wretched': 14960,\n",
       " 'sickness': 12039,\n",
       " 'make': 8027,\n",
       " 'spared': 12429,\n",
       " 'grief': 5814,\n",
       " 'concealing': 2369,\n",
       " 'extent': 4611,\n",
       " 'disorder': 3599,\n",
       " 'father s': 4823,\n",
       " 'long journey': 7793,\n",
       " 'shadowy': 11854,\n",
       " 'solitude': 12327,\n",
       " 'longing': 7810,\n",
       " 'light': 7614,\n",
       " 'frantic': 5253,\n",
       " 'rest': 11006,\n",
       " 'lifted': 7610,\n",
       " 'entreating': 4323,\n",
       " 'hands': 5970,\n",
       " 'ruined': 11304,\n",
       " 'tower': 13683,\n",
       " 'forest': 5173,\n",
       " 'unknown': 14058,\n",
       " 'outer': 9339,\n",
       " 'second': 11698,\n",
       " 'brilliant': 1493,\n",
       " 'air': 297,\n",
       " 'art': 660,\n",
       " 'wonders': 14880,\n",
       " 'said second': 11472,\n",
       " 'prime': 10190,\n",
       " 'festivals': 4938,\n",
       " 'held': 6172,\n",
       " 'weary': 14623,\n",
       " 'talking': 13197,\n",
       " 'dreaming': 3832,\n",
       " 'perdita s': 9645,\n",
       " 's cottage': 11356,\n",
       " 'perdita s cottage': 9646,\n",
       " 'months': 8539,\n",
       " 'eve': 4418,\n",
       " 'talk': 13194,\n",
       " 'queer': 10479,\n",
       " 'earth': 3981,\n",
       " 'noises': 8949,\n",
       " 'clear': 2100,\n",
       " 'arkham': 627,\n",
       " 'night': 8902,\n",
       " 'lately': 7405,\n",
       " 'community': 2309,\n",
       " 'dead': 3022,\n",
       " 'burnt': 1594,\n",
       " 'order': 9287,\n",
       " 'prevent': 10172,\n",
       " 'injurious': 6827,\n",
       " 'public': 10388,\n",
       " 'peace': 9592,\n",
       " 'imagine': 6548,\n",
       " 'point': 9936,\n",
       " 'view': 14371,\n",
       " 'section': 11715,\n",
       " 'point view': 9939,\n",
       " 'wilbur': 14755,\n",
       " 'growing': 5846,\n",
       " 'uncannily': 13935,\n",
       " 'looked': 7815,\n",
       " 'boy': 1415,\n",
       " 'entered': 4290,\n",
       " 'year': 15010,\n",
       " 'looked like': 7817,\n",
       " 'checked': 1946,\n",
       " 'anxiety': 505,\n",
       " 'doomed': 3733,\n",
       " 'toil': 13606,\n",
       " 'mines': 8390,\n",
       " 'unwholesome': 14158,\n",
       " 'trade': 13703,\n",
       " 'artist': 679,\n",
       " 'favourite': 4840,\n",
       " 'employment': 4176,\n",
       " 'kinds': 7237,\n",
       " 'o': 9032,\n",
       " 'cities': 2043,\n",
       " 'sea': 11664,\n",
       " 'island': 7050,\n",
       " 'heaved': 6139,\n",
       " 'thar': 13347,\n",
       " 'irony': 7038,\n",
       " 'ryland': 11339,\n",
       " 'roused': 11262,\n",
       " 'resistance': 10974,\n",
       " 'asserted': 730,\n",
       " 'permitted': 9683,\n",
       " 'encrease': 4201,\n",
       " 'party': 9520,\n",
       " 'indulgence': 6750,\n",
       " 'sweep': 13091,\n",
       " 'away': 900,\n",
       " 'cobwebs': 2193,\n",
       " 'blinded': 1274,\n",
       " 'countrymen': 2697,\n",
       " 'came': 1667,\n",
       " 'gently': 5496,\n",
       " 'stealthily': 12645,\n",
       " 'attained': 797,\n",
       " 'appreciation': 563,\n",
       " 'spirit': 12494,\n",
       " 'length': 7531,\n",
       " 'properly': 10318,\n",
       " 'entertain': 4302,\n",
       " 'figures': 4970,\n",
       " 'judges': 7142,\n",
       " 'vanished': 14255,\n",
       " 'tall': 13199,\n",
       " 'candles': 1692,\n",
       " 'sank': 11510,\n",
       " 'nothingness': 8995,\n",
       " 'flames': 5036,\n",
       " 'went': 14655,\n",
       " 'utterly': 14213,\n",
       " 'blackness': 1243,\n",
       " 'darkness': 2968,\n",
       " 'supervened': 12992,\n",
       " 'sensations': 11767,\n",
       " 'swallowed': 13079,\n",
       " 'mad': 7961,\n",
       " 'rushing': 11330,\n",
       " 'descent': 3275,\n",
       " 'soul': 12386,\n",
       " 'carriage': 1750,\n",
       " 'measured': 8227,\n",
       " 'express': 4590,\n",
       " 'rectangular': 10723,\n",
       " 'precision': 10101,\n",
       " 'attending': 809,\n",
       " 'movement': 8639,\n",
       " 'observed': 9089,\n",
       " 'diminutive': 3491,\n",
       " 'figure': 4967,\n",
       " 'affectation': 228,\n",
       " 'constraint': 2512,\n",
       " 'noticed': 8998,\n",
       " 'gentleman': 5489,\n",
       " 'dimensions': 3486,\n",
       " 'readily': 10641,\n",
       " 'account': 83,\n",
       " 'reserve': 10963,\n",
       " 'hauteur': 6054,\n",
       " 'sense': 11768,\n",
       " 'dignity': 3474,\n",
       " 'colossal': 2237,\n",
       " 'proportion': 10325,\n",
       " 'wholly': 14733,\n",
       " 'sorry': 12378,\n",
       " 'loss': 7851,\n",
       " 'abysses': 53,\n",
       " 'closely': 2144,\n",
       " 'written': 14979,\n",
       " 'sheets': 11908,\n",
       " 'explained': 4573,\n",
       " 'erich': 4358,\n",
       " 'zann': 15092,\n",
       " 'erich zann': 4359,\n",
       " 'pulled': 10400,\n",
       " 'steel': 12649,\n",
       " 'claws': 2093,\n",
       " 'neck': 8830,\n",
       " 'dragged': 3797,\n",
       " 'beldame': 1113,\n",
       " 'edge': 4024,\n",
       " 'gulf': 5880,\n",
       " 'access': 61,\n",
       " 'closed': 2140,\n",
       " 'lovely': 7881,\n",
       " 'child': 1975,\n",
       " 'nearly': 8820,\n",
       " 'years': 15015,\n",
       " 'nearly years': 8823,\n",
       " 'years age': 15016,\n",
       " 'hoary': 6276,\n",
       " 'forth': 5207,\n",
       " 'hand': 5955,\n",
       " 'helped': 6180,\n",
       " 'olney': 9212,\n",
       " 'host': 6351,\n",
       " 'vast': 14283,\n",
       " 'shell': 11910,\n",
       " 'conches': 2386,\n",
       " 'wild': 14761,\n",
       " 'awesome': 905,\n",
       " 'clamour': 2070,\n",
       " 'uncle': 13943,\n",
       " 'lay': 7435,\n",
       " 'carelessly': 1738,\n",
       " 'dug': 3908,\n",
       " 'open': 9232,\n",
       " 'pit': 9833,\n",
       " 'angry': 451,\n",
       " 'framed': 5245,\n",
       " 'straggling': 12731,\n",
       " 'locks': 7761,\n",
       " 'cornered': 2639,\n",
       " 'hats': 6046,\n",
       " 'cornered hats': 2640,\n",
       " 'aware': 899,\n",
       " 'apparent': 528,\n",
       " 'paradox': 9471,\n",
       " 'occasioned': 9110,\n",
       " 'visual': 14436,\n",
       " 'area': 610,\n",
       " 'susceptible': 13062,\n",
       " 'feeble': 4871,\n",
       " 'impressions': 6637,\n",
       " 'exterior': 4612,\n",
       " 'portions': 10007,\n",
       " 'retina': 11038,\n",
       " 'mercy': 8310,\n",
       " 'known': 7295,\n",
       " 'hill': 6248,\n",
       " 'remote': 10862,\n",
       " 'bit': 1220,\n",
       " 'backwoods': 936,\n",
       " 'seat': 11688,\n",
       " 'uncomfortable': 13948,\n",
       " 'superstitions': 12990,\n",
       " 'known better': 7296,\n",
       " 'balloon': 955,\n",
       " 'ascensions': 693,\n",
       " 'considerable': 2485,\n",
       " 'height': 6165,\n",
       " 'pain': 9413,\n",
       " 'respiration': 11000,\n",
       " 'uneasiness': 13994,\n",
       " 'experienced': 4563,\n",
       " 'accompanied': 67,\n",
       " 'bleeding': 1261,\n",
       " 'nose': 8984,\n",
       " 'symptoms': 13145,\n",
       " 'alarming': 307,\n",
       " 'inconvenient': 6704,\n",
       " 'altitude': 376,\n",
       " 'great uneasiness': 5775,\n",
       " 'circulation': 2033,\n",
       " 'property': 10320,\n",
       " 'supported': 13002,\n",
       " 'wants': 14547,\n",
       " 'society': 12293,\n",
       " 'sudden': 12898,\n",
       " 'hideous': 6224,\n",
       " 'boundaries': 1395,\n",
       " 'private': 10213,\n",
       " 'possession': 10022,\n",
       " 'thrown': 13495,\n",
       " 'products': 10255,\n",
       " 'human': 6412,\n",
       " 'present': 10130,\n",
       " 'existing': 4540,\n",
       " 'far': 4770,\n",
       " 'thinned': 13405,\n",
       " 'generation': 5475,\n",
       " 'possibly': 10029,\n",
       " 'consume': 2523,\n",
       " 'tried': 13791,\n",
       " 'organ': 9297,\n",
       " 'considered': 2490,\n",
       " 'cavities': 1816,\n",
       " 'result': 11022,\n",
       " 'action': 122,\n",
       " 'water': 14584,\n",
       " 'believed': 1123,\n",
       " 'america': 387,\n",
       " 'taint': 13176,\n",
       " 'yellow': 15031,\n",
       " 'fever': 4942,\n",
       " 'epidemic': 4335,\n",
       " 'gifted': 5526,\n",
       " 'virulence': 14414,\n",
       " 'unexplored': 14006,\n",
       " 'unfrequently': 14020,\n",
       " 'visited': 14428,\n",
       " 'recess': 10693,\n",
       " 'amid': 393,\n",
       " 'woods': 14889,\n",
       " 'groves': 5844,\n",
       " 'moment': 8495,\n",
       " 'imagined': 6549,\n",
       " 'evening': 4421,\n",
       " 'mother': 8597,\n",
       " 'sister': 12132,\n",
       " 'arrived': 653,\n",
       " 'mother sister': 8603,\n",
       " 'consented': 2478,\n",
       " 'request': 10943,\n",
       " 'taking': 13184,\n",
       " 'followed': 5123,\n",
       " 'wonderful': 14874,\n",
       " 'ingenious': 6802,\n",
       " 'add': 144,\n",
       " 'mr': 8643,\n",
       " 'believe': 1120,\n",
       " 'useful': 14194,\n",
       " 'mechanical': 8231,\n",
       " 'contrivances': 2587,\n",
       " 'daily': 2916,\n",
       " 'springing': 12532,\n",
       " 'ah': 284,\n",
       " 'sure': 13018,\n",
       " 'needless': 8838,\n",
       " 'general': 5465,\n",
       " 'smith': 12259,\n",
       " 'heightened': 6168,\n",
       " 'exalted': 4462,\n",
       " 'opinion': 9260,\n",
       " 'valuable': 14247,\n",
       " 'enjoy': 4262,\n",
       " 'invention': 7009,\n",
       " 'needless say': 8839,\n",
       " 'legs': 7527,\n",
       " 'portion': 10002,\n",
       " 'story': 12727,\n",
       " 'ethelred': 4402,\n",
       " 'hero': 6205,\n",
       " 'having': 6056,\n",
       " 'sought': 12383,\n",
       " 'vain': 14229,\n",
       " 'admission': 176,\n",
       " 'dwelling': 3942,\n",
       " 'hermit': 6204,\n",
       " 'proceeds': 10236,\n",
       " 'good': 5653,\n",
       " 'entrance': 4318,\n",
       " 'obviously': 9105,\n",
       " 'prominent': 10295,\n",
       " 'incongruities': 6696,\n",
       " 'deductions': 3117,\n",
       " 'shall': 11863,\n",
       " 'lead': 7454,\n",
       " 'truth': 13831,\n",
       " 'receive': 10685,\n",
       " 'hermann': 6203,\n",
       " 'letter': 7562,\n",
       " 'matter': 8202,\n",
       " 'conversation': 2596,\n",
       " 'inner': 6839,\n",
       " 'everlasting': 4434,\n",
       " 'treatise': 13767,\n",
       " 'duelli': 3904,\n",
       " 'lex': 7571,\n",
       " 'scripta': 11645,\n",
       " 'et': 4393,\n",
       " 'non': 8953,\n",
       " 'aliterque': 333,\n",
       " 'matter course': 8203,\n",
       " 'duelli lex': 3905,\n",
       " 'lex scripta': 7572,\n",
       " 'scripta et': 11646,\n",
       " 'et non': 4394,\n",
       " 'non aliterque': 8954,\n",
       " 'duelli lex scripta': 3906,\n",
       " 'lex scripta et': 7573,\n",
       " 'scripta et non': 11647,\n",
       " 'et non aliterque': 4395,\n",
       " 'wyatt': 14988,\n",
       " 'rooms': 11238,\n",
       " 'cabin': 1634,\n",
       " 'separated': 11791,\n",
       " 'main': 8008,\n",
       " 'door': 3734,\n",
       " 'locked': 7755,\n",
       " 'wyatt s': 14989,\n",
       " 'door locked': 3746,\n",
       " 'pursued': 10445,\n",
       " 'enemy': 4236,\n",
       " 'wings': 14806,\n",
       " 'feet': 4884,\n",
       " 'flew': 5058,\n",
       " 'apartment': 514,\n",
       " 'dismissed': 3597,\n",
       " 'attendants': 807,\n",
       " 'threw': 13475,\n",
       " 'wildly': 14769,\n",
       " 'floor': 5077,\n",
       " 'blood': 1284,\n",
       " 'suppress': 13012,\n",
       " 'shrieks': 11996,\n",
       " 'prey': 10180,\n",
       " 'vulture': 14487,\n",
       " 'striving': 12796,\n",
       " 'multitudinous': 8685,\n",
       " 'ideas': 6498,\n",
       " 'horrid': 6337,\n",
       " 'furies': 5346,\n",
       " 'cruel': 2818,\n",
       " 'poured': 10047,\n",
       " 'swift': 13107,\n",
       " 'succession': 12892,\n",
       " 'wound': 14941,\n",
       " 'worked': 14908,\n",
       " 'devil': 3359,\n",
       " 'vow': 14480,\n",
       " 'vengeance': 14318,\n",
       " 'devote': 3370,\n",
       " 'fiend': 4956,\n",
       " 'torture': 13664,\n",
       " 'involuntary': 7022,\n",
       " 'got': 5684,\n",
       " 'oven': 9356,\n",
       " 'alive': 334,\n",
       " 'certainly': 1867,\n",
       " 'turn': 13862,\n",
       " 'scrutinizing': 11654,\n",
       " 'machinery': 7959,\n",
       " 'moving': 8642,\n",
       " 'mechanism': 8234,\n",
       " 'changed': 1895,\n",
       " 'position': 10014,\n",
       " 'accounted': 85,\n",
       " 'simple': 12087,\n",
       " 'laws': 7434,\n",
       " 'perspective': 9711,\n",
       " 'subsequent': 12869,\n",
       " 'examinations': 4465,\n",
       " 'convinced': 2609,\n",
       " 'undue': 13988,\n",
       " 'alterations': 370,\n",
       " 'mirrors': 8416,\n",
       " 'trunk': 13826,\n",
       " 'organic': 9298,\n",
       " 'tended': 13288,\n",
       " 'awake': 893,\n",
       " 'vague': 14227,\n",
       " 'memories': 8272,\n",
       " 'conscious': 2474,\n",
       " 'idea': 6495,\n",
       " 'mockingly': 8470,\n",
       " 'resembled': 10957,\n",
       " 'suggested': 12922,\n",
       " 'pity': 9846,\n",
       " 'charmion': 1927,\n",
       " 'majesty': 8024,\n",
       " 'speculative': 12469,\n",
       " 'merged': 8317,\n",
       " 'august': 847,\n",
       " 'oh god': 9159,\n",
       " 'told': 13612,\n",
       " 'eighty': 4066,\n",
       " 'tragedies': 13709,\n",
       " 'ninety': 8927,\n",
       " 'speeches': 12472,\n",
       " 'treatises': 13768,\n",
       " 'eighth': 4065,\n",
       " 'book': 1360,\n",
       " 'hymns': 6479,\n",
       " 'homer': 6300,\n",
       " 'junior': 7155,\n",
       " 'refused': 10762,\n",
       " 'dropped': 3867,\n",
       " 'later': 7408,\n",
       " 'moment later': 8498,\n",
       " 'reply': 10914,\n",
       " 'know': 7281,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래와 같은 feature들로 대부분 시작한다.\n",
    "# TfidfVectorizer : 단어 카운트 가중치 \n",
    "# min_df : DF(document-frequency)의 최소 빈도값 설정, DF는 특정 단어가 나타나는 '문서의 수'를 의미,단어의 수가 아니라!, 3이면 1,2인 것들을 탈락함 \n",
    "# analyzer : 'word', 'char' 중 선택\n",
    "# sublinear_tf : TF(Term-Freqeuncy,단어빈도) 값의 스무딩 여부를 결정하는 파라미터 T/F , 높은 TF값을 완만하게 처리하는 효과. 아웃라이어가 너무 심한 경우 사용\n",
    "# ngram_range : 단어의 묶음, ex) very good은 두 단어가 묶여야 정확한 의미가 살아난다.\n",
    "# max_features : tf-idf vector의 최대 feature를 설정해주는 것, tf-idf 벡터는 단어사전의 인덱스만큼 feature를 부여받음. 종류의 숫자를 제한.\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                      token_pattern=r'\\w{1,}', ngram_range=(1,3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                      stop_words='english')\n",
    "# training, test set 둘다 TF-IDF 로 fit하기, 벡터라이저가 단어들을 학습시킨다. \n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "tfv.vocabulary_ # 벡터라이저가 학습한 단어사전을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:34.737470Z",
     "start_time": "2021-01-13T08:04:33.744652Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xtrain_tfv = tfv.transform(xtrain)\n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:36.893371Z",
     "start_time": "2021-01-13T08:04:34.739456Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.572 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hanbit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# TF IDF로 간단한 Logistic Regression Fit하기.\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print(\"logloss: %0.3f \" %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "우리는 multiclass logloss로 0.572의 손실을 얻었다.\n",
    "\n",
    "### counter 변수 사용. Logistic Regression\n",
    "- 더 좋은 점수를 얻기 위해 다른 데이터로 동일한 모델을 적용해본다.\n",
    "- TF-IDF를 사용하기 전에, 우리는 feature로써 word count를 사용할 수 있다.\n",
    "- 이것은 scikit-learn의 CountVectorizer로 쉽게 사용할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:04:40.778712Z",
     "start_time": "2021-01-13T08:04:36.895368Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CountVectorizer : 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩 벡터를 만든다.\n",
    "ctv = CountVectorizer(analyzer='word', token_pattern= r'\\w{1,}', ngram_range=(1,3), stop_words='english')\n",
    "\n",
    "# training, test 데이터셋 모두 fit count vectorizer fit하자.\n",
    "\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv = ctv.transform(xtrain)\n",
    "xvalid_ctv = ctv.transform(xvalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:05:09.847045Z",
     "start_time": "2021-01-13T08:04:40.780717Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.527 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hanbit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf  = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv,ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print(\"logloss: %0.3f \" %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "0.527 의 손실로, 더 나아지진 않았다.\n",
    "\n",
    "### Naive-Bayes\n",
    "다음으로는 에전에 유명했던 간단한 모델인 Naive-Bayes를 사용해보자.\n",
    "우선 두 데이터 셋으로 naive-bayes를 적용하면 어떻게 되는지 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:05:09.872960Z",
     "start_time": "2021-01-13T08:05:09.851031Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print('logloss: %0.3f' % multiclass_logloss(yvalid, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "위의 두 모델보보는 훨씬 더 좋아졌다.\n",
    "하지만, 여전히 logistic regression이 훨씬 더 좋다. 이 모델을 count data로 대신 사용하였을때는 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:05:09.996544Z",
     "start_time": "2021-01-13T08:05:09.875950Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print(\"logloss: %0.3f\" % multiclass_logloss(yvalid, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "더 나아진 것 같진 않다. 오래된 모델이 더 작동하는 것 가다.\n",
    "\n",
    "### SVM\n",
    "이번엔 SVM을 적용해 본다. SVM은 시간이 많이 걸리므로 SVM 적용 전에 특이값 분해(Singular Value Decomposition)를 사용하여\n",
    "TF-IDF로부터 feature의 갯수를 줄여준다.\n",
    "\n",
    "또, SVM을 적용하기 전에 데이터를 표준화해 한다는 점을 주의해아한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:05:12.346737Z",
     "start_time": "2021-01-13T08:05:10.002525Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# from SVD로 부터 데이터를 얻어 스케일링하기. 변수명은 다시 사용하여 짓는다.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:13:52.700051Z",
     "start_time": "2021-01-13T08:05:12.347733Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.732\n"
     ]
    }
   ],
   "source": [
    "# 자 이제 SVM을 돌려보자.\n",
    "clf = SVC(C=1.0, probability=True)\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print(\"logloss: %0.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### xgboost\n",
    "그 다음은 캐글에서 인기 있는 xgboost를 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:14:13.572533Z",
     "start_time": "2021-01-13T08:13:52.702002Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.781 \n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8,\n",
    "                        nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print(\"logloss: %0.3f \"% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:14:13.609920Z",
     "start_time": "2021-01-13T08:14:13.576520Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17621x400266 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 556265 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_ctv.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:16:34.999750Z",
     "start_time": "2021-01-13T08:14:13.613906Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.772\n"
     ]
    }
   ],
   "source": [
    "# 같은 알고리즘으로 몇번씩 돌려보자.\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "                       subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print(\"logloss: %0.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:19:01.440041Z",
     "start_time": "2021-01-13T08:16:35.007723Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.772\n"
     ]
    }
   ],
   "source": [
    "# 같은 알고리즘으로 몇번씩 돌려보자.\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "                       subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print(\"logloss: %0.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:19:53.920057Z",
     "start_time": "2021-01-13T08:19:01.444029Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss : 0.796\n"
     ]
    }
   ],
   "source": [
    "# 이번엔 nthread 옵션만 넣어보자.\n",
    "\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions=clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print(\"logloss : %0.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "결과를 보면 xgboost는 운이 없는거 같다. 하지만 정확한 결과는 아니다. 왜냐하면 하이퍼 파라미터 최적화를 진행하지 않았기 때문이다. 지금부터 다뤄보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Grid Search\n",
    "그리드 search는 하이퍼 파리미터 최적화 기법이다. 그리 효과적이진 않지만 사용하려는 그리드를 알고 있으면 좋은 결과를 얻을 수 있다. 이 [포스트](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)를 참고하여 일반적으로 사용해야하는 매개 변수를 지정하여 사용한다. \n",
    "\n",
    "주로 사용하는 파라미터는 기억하면 좋다. 최적화를 위한 많은 하이퍼 파라미터는 효과적일 수도 있고 아닐 수도 있다. 이번 섹션에서는 logistic regression을 사용하여 grid search에 대하여 말해볼 것이다. grid search를 시작하기전에, scoring 함수를 만드는 것이 \n",
    "필요하다. scikit-learn의 함수인 make_scorer 를 사용하여 만들어보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:19:53.931781Z",
     "start_time": "2021-01-13T08:19:53.924045Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "다음으로는 pipeline이 필요하다. 여기 예제에서는 SVD,scaling, logistic regression으로 구성된 파이프라인을 사용할 것이다. 파이프라인에 있는 모듈을 하나만 사용하는 것보다 더 많이 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:45:44.804617Z",
     "start_time": "2021-01-13T08:45:44.799619Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SVD 초기화\n",
    "svd = TruncatedSVD()\n",
    "# standard scaler 초기화\n",
    "scl = preprocessing.StandardScaler()\n",
    "# 여기선 logistic regression을 사용할 것이다.\n",
    "lr_model = LogisticRegression()\n",
    "# 파이프라인 만들기 \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                        ('scl',scl),\n",
    "                        ('lr', lr_model)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 grid 파라미터가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:46:19.465136Z",
     "start_time": "2021-01-13T08:46:19.461111Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120,180],\n",
    "             'lr__C' : [0.1, 1.0, 10],\n",
    "             'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD의 경우 120, 180 성분을 평가하고 로지스틱 회귀 분석의 경우 L1 및 L2 패널티를 사용하여 C의 세 가지 값을 평가한다. \n",
    "이제 이러한 매개 변수에 대한 그리드 검색을 시작할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:34:59.828675Z",
     "start_time": "2021-01-13T10:34:57.173960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    2.3s remaining:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    2.4s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    2.4s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    2.4s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    2.4s finished\n",
      "c:\\users\\hanbit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:849: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T02:06:52.161740Z",
     "start_time": "2021-01-13T02:06:28.671317Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "svm과 비슷한 점수를 받았다. 이 기술은 아래와 같이 xgboost 또한 multinomial naive bayesas 을 미세 조정하는데도 사용할 수 있다.\n",
    "여기서는 tf-idf에서 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:29:47.482881Z",
     "start_time": "2021-01-13T10:29:44.348922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    2.8s remaining:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    2.8s remaining:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    2.8s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    2.9s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    2.9s finished\n",
      "c:\\users\\hanbit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:849: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아마 결과는 순수 naive bayes 점수보다 8% 개선 됬을 것이다. \n",
    "NLP 문제에서는 단어 벡터를 보는 것이 일반적이다. 단어 벡터는 데이터에 대한 많은 통찰력을 제공한다. 자세히 실습을 진행하고 싶으면 [kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)에서 참고해보자.\n",
    "여기선 바로 딥러닝으로 들어가보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:48:17.891787Z",
     "start_time": "2021-01-13T10:48:17.887798Z"
    }
   },
   "source": [
    "### WordVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:59:55.371307Z",
     "start_time": "2021-01-13T10:55:13.234258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [04:42, 7783.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195892 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../data/glove.840B.300d.txt', encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "지금은 딥러닝의 시대이다. 우리는 신경망 네트워크를 학습하지않고는 살아갈 수 없다. 여기에 LSTM과 간단한 dense network를 활용하여 GloVe 변수를 학습한다. \n",
    "자 dense network를 첫번째로 시작해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:33:42.443639Z",
     "start_time": "2021-01-13T08:33:42.436648Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HANBIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "#     words = str(s).lower().decode('utf-8')\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:33:50.606208Z",
     "start_time": "2021-01-13T08:33:45.286137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 17621/17621 [00:04<00:00, 3725.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1958/1958 [00:00<00:00, 3372.05it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:35:46.175702Z",
     "start_time": "2021-01-13T10:35:45.951369Z"
    }
   },
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:36:10.372705Z",
     "start_time": "2021-01-13T10:36:10.366734Z"
    }
   },
   "outputs": [],
   "source": [
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:39:39.119859Z",
     "start_time": "2021-01-13T10:39:38.987235Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:40:32.978082Z",
     "start_time": "2021-01-13T10:40:27.395681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 1.0885 - val_loss: 1.0876\n",
      "Epoch 2/5\n",
      "276/276 [==============================] - 1s 3ms/step - loss: 1.0880 - val_loss: 1.0877\n",
      "Epoch 3/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 1.0881 - val_loss: 1.0878\n",
      "Epoch 4/5\n",
      "276/276 [==============================] - 1s 3ms/step - loss: 1.0881 - val_loss: 1.0878\n",
      "Epoch 5/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 1.0881 - val_loss: 1.0884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f28e2b19c8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y= ytrain_enc, batch_size=64, epochs=5, verbose=1,\n",
    "         validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 나은 결과를 얻기 위해서는 신경 네트워크의 매개 변수를 계속해서 조정하고, 더 많은 레이어를 추가하고, dropout을 늘려야 한다.\n",
    "하지만 신경네트워크는 최적화 없이도 구현 및 실행이 빠르며 xgboost보다 더 나은 결과를 얻을 수 있다는 것을 확인할 수 있다.\n",
    "\n",
    "더 나아가려면 LSTM을 사용하여 텍스트 데이터를 토큰화해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:44:42.799729Z",
     "start_time": "2021-01-13T10:44:41.895020Z"
    }
   },
   "outputs": [],
   "source": [
    "# 케라스 사용하여 토큰화하기\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T11:00:56.041574Z",
     "start_time": "2021-01-13T11:00:55.948876Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 25943/25943 [00:00<00:00, 306031.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T11:01:48.915560Z",
     "start_time": "2021-01-13T11:01:48.610529Z"
    }
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T13:17:25.453776Z",
     "start_time": "2021-01-13T11:02:36.449256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 61s 2s/step - loss: 1.0650 - val_loss: 0.9455\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 65s 2s/step - loss: 0.9160 - val_loss: 0.7598\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.8155 - val_loss: 0.7086\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.7866 - val_loss: 0.6971\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.7670 - val_loss: 0.6734\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.7370 - val_loss: 0.6497\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.7283 - val_loss: 0.6542\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.7057 - val_loss: 0.6409\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.6858 - val_loss: 0.6169\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.6722 - val_loss: 0.5906\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.6454 - val_loss: 0.6014\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.6291 - val_loss: 0.5930\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.6134 - val_loss: 0.5683\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.5853 - val_loss: 0.5478\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.5830 - val_loss: 0.5467\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.5651 - val_loss: 0.5351\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.5450 - val_loss: 0.5247\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.5430 - val_loss: 0.5179\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.5278 - val_loss: 0.5287\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.5114 - val_loss: 0.5227\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.5010 - val_loss: 0.5438\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.4936 - val_loss: 0.5302\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.4827 - val_loss: 0.5128\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.4700 - val_loss: 0.5106\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.4727 - val_loss: 0.5080\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.4523 - val_loss: 0.5093\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.4435 - val_loss: 0.4928\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.4363 - val_loss: 0.5070\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.4356 - val_loss: 0.4948\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.4299 - val_loss: 0.5202\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.4138 - val_loss: 0.5015\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.4090 - val_loss: 0.4953\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.4004 - val_loss: 0.5007\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3959 - val_loss: 0.5078\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3970 - val_loss: 0.5134\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3858 - val_loss: 0.5027\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 88s 3s/step - loss: 0.3715 - val_loss: 0.5112\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 88s 3s/step - loss: 0.3702 - val_loss: 0.5005\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 82s 2s/step - loss: 0.3692 - val_loss: 0.5119\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3644 - val_loss: 0.4920\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3574 - val_loss: 0.5118\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3527 - val_loss: 0.5238\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3464 - val_loss: 0.5599\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.3429 - val_loss: 0.4988\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 87s 2s/step - loss: 0.3304 - val_loss: 0.5184\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 87s 2s/step - loss: 0.3303 - val_loss: 0.5414\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 86s 2s/step - loss: 0.3256 - val_loss: 0.5400\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 89s 3s/step - loss: 0.3325 - val_loss: 0.5493\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 86s 2s/step - loss: 0.3227 - val_loss: 0.5107\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 89s 3s/step - loss: 0.3122 - val_loss: 0.5299\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.3088 - val_loss: 0.5238\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 83s 2s/step - loss: 0.3141 - val_loss: 0.5299\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 90s 3s/step - loss: 0.3148 - val_loss: 0.5105\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.3032 - val_loss: 0.5315\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2961 - val_loss: 0.5659\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.3027 - val_loss: 0.5749\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.2878 - val_loss: 0.5299\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.2955 - val_loss: 0.5514\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2784 - val_loss: 0.5671\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2771 - val_loss: 0.5685\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2769 - val_loss: 0.5415\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2736 - val_loss: 0.5809\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2710 - val_loss: 0.5657\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2658 - val_loss: 0.5644\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2601 - val_loss: 0.5429\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2642 - val_loss: 0.5673\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2594 - val_loss: 0.5776\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2684 - val_loss: 0.5378\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2513 - val_loss: 0.5800\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2608 - val_loss: 0.5649\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2575 - val_loss: 0.5743\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2472 - val_loss: 0.5785\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2467 - val_loss: 0.5946\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2508 - val_loss: 0.5819\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2391 - val_loss: 0.5895\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2457 - val_loss: 0.6126\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2524 - val_loss: 0.5570\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2491 - val_loss: 0.5488\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2363 - val_loss: 0.5979\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.2369 - val_loss: 0.5644\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2302 - val_loss: 0.5835\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2288 - val_loss: 0.5836\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 79s 2s/step - loss: 0.2361 - val_loss: 0.5878\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2250 - val_loss: 0.5866\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2225 - val_loss: 0.5966\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2188 - val_loss: 0.5691\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2305 - val_loss: 0.5506\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2175 - val_loss: 0.5933\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2142 - val_loss: 0.6137\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 81s 2s/step - loss: 0.2165 - val_loss: 0.6676\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2192 - val_loss: 0.5979\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2165 - val_loss: 0.6181\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2194 - val_loss: 0.6285\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2126 - val_loss: 0.6091\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 70s 2s/step - loss: 0.1987 - val_loss: 0.6469\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2122 - val_loss: 0.6011\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2060 - val_loss: 0.6016\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2118 - val_loss: 0.6209\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2053 - val_loss: 0.6138\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2063 - val_loss: 0.6295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f39510ddc8>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100,\n",
    "         verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 매우 가까워져 간다. GRU 두개 레이어를 더 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 300\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=max_len,\n",
    "                   trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0,\n",
    "                         mode='auto')\n",
    "model.fit(xtrain_pad, y =ytrain_enc, batch_size=512,epochs=100,\n",
    "         verbose=1,validation_data=(xvalid_pad, yvalid_enc), callback=[earlystop])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "결과, 우리가 이전에 가졌던것보다 훨씬더 좋아진다. 계속 최적화하면 성능이 향상될 것이다. \n",
    "stemming, lemmatization는 충분히 시도할 만하다. (지금은 진행하진 않겠지만..)\n",
    "\n",
    "캐글에서는 최고 점수를 받으려면 앙상블 모델이 있어야한다. 앙상블을 확인해보자.\n",
    " 이것은 이 [사이트](https://github.com/abhishekkrthakur/pysembler)를 참고하면된다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
